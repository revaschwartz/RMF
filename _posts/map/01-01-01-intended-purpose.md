---
date: 01-01-01
title: MAP 1.1
categories:
  - MAP-1
description: Intended purpose, setting in which the AI system will be deployed, the specific set of users along with their expectations, and impacts of system use are understood and documented. Assumptions and related limitations about AI system purpose and use are enumerated. 
type: Map
order_number: 1
---
{::options parse_block_html="true" /} 


<details>
<summary markdown="span">**What is this subcategory about?**</summary>      
<br>
Mapping context may include examination of the following:
* intended and actual deployment setting.
* specific set of users. 
* operator or subject expectations. 
* concept of operations. 
* intended purpose and impact of system use. 
* requirements for system deployment and operation. 
* potential negative impacts to individuals, groups, communities, organizations, and society – or context-specific impacts such as legal requirements or impacts to the environment. 
* unintended, downstream, or other unknown contextual factors.

</details>

<details>
<summary markdown="span">**How can organizations achieve the outcomes of this subcategory?**</summary>

* Pursue AI system design purposefully, after non-AI solutions are considered. 
* Define and document the task, purpose, minimum functionality, and benefits of the AI system to inform considerations about whether the project is worth pursuing.
* Maintain awareness of industry, technical, and applicable legal standards.
* Collaboratively consider intended AI system design tasks along with unanticipated purposes.
* Determine the user and organizational requirements, including business and technical requirements.
* Define the AI system context of use, including:
    * operational environment
    * impacts to individuals, groups, communities, organizations, and society
    * user characteristics and tasks
    * social environment.
* Track and document existing AI systems held by the organization, and those maintained or supported by third-party entities.
* Gain and maintain awareness about evaluating scientific claims related to AI system performance and benefits before launching into system design.
* Identify human-AI interaction and/or roles, such as whether the application will support or replace human decision making. 
* Plan for risks related to human-AI configurations, and document requirements, roles, and responsibilities for human oversight of deployed systems.

</details>

<details>
<summary markdown="span">**What are the transparency and documentation considerations?**</summary>         
                     
**Transparency Considerations – Key Questions: MAP 1.1**
* Who is ultimately responsible for the decisions of the AI and is this person aware of the intended uses and limitations of the analytic?
* Who will be responsible for maintaining, re-verifying, monitoring, and updating this AI once deployed?
* Who is accountable for the ethical considerations during all stages of the AI lifecycle?
* Why was the dataset created? (e.g., were there specific tasks in mind, or a specific gap that needed to be filled?
* How does the entity ensure that the data collected are adequate, relevant, and not excessive in relation to the intended purpose?

**AI Transparency Resources: MAP 1.1**
* Datasheets for Datasets
* GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities
* “Stakeholders in Explainable AI,” Sep. 2018, [Online]. [link](http://arxiv.org/abs/1810.00184)

</details>

<details>
<summary markdown="span">**What are some informative references?**</summary>      
<br>
**Socio-technical systems**

Andrew D. Selbst, Danah Boyd, Sorelle A. Friedler, et al. 2019. Fairness and Abstraction in Sociotechnical Systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* '19). Association for Computing Machinery, New York, NY, USA, 59–68. https://doi.org/10.1145/3287560.3287598

**Problem formulation**

Roel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial intelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004-3702. DOI: https://doi.org/10.1016/j.artint.2021.103555

Samir Passi and Solon Barocas. 2019. Problem Formulation and Fairness. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* '19). Association for Computing Machinery, New York, NY, USA, 39–48. https://doi.org/10.1145/3287560.3287567

**Context mapping**

Emilio Gómez-González and Emilia Gómez. 2020. Artificial intelligence in medicine and healthcare. Joint Research Centre (European Commission). Retrieved from https://op.europa.eu/en/publication-detail/-/publication/b4b5db47-94c0-11ea-aac4-01aa75ed71a1/language-en

Sarah Spiekermann and Till Winkler. 2020. Value-based Engineering for Ethics by Design. arXiv:2004.13676. Retrieved from https://arxiv.org/abs/2004.13676

Social Impact Lab. 2017. Framework for Context Analysis of Technologies in Social Change Projects (Draft v2.0). Retrieved from https://www.alnap.org/system/files/content/resource/files/main/Draft%20SIMLab%20Context%20Analysis%20Framework%20v2.0.pdf

Solon Barocas, Asia J. Biega, Margarita Boyarskaya, et al. 2021. Responsible computing during COVID-19 and beyond. Commun. ACM 64, 7 (July 2021), 30–32. https://doi.org/10.1145/3466612

**Identification of harms**

Harini Suresh and John V. Guttag. 2020. A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle. arXiv:1901.10002. Retrieved from https://arxiv.org/abs/1901.10002

Margarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of Imagination in AI Infused System Development and Deployment. arXiv:2011.13416. Retrieved from https://arxiv.org/abs/2011.13416

https://docs.microsoft.com/en-us/azure/architecture/guide/responsible-innovation/harms-modeling/

**Measurement and evaluation**

Abigail Z. Jacobs and Hanna Wallach. 2021. Measurement and Fairness. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT '21). Association for Computing Machinery, New York, NY, USA, 375–385. https://doi.org/10.1145/3442188.3445901

Ben Hutchinson, Negar Rostamzadeh, Christina Greer, et al. 2022. Evaluation Gaps in Machine Learning Practice. arXiv:2205.05256. Retrieved from https://arxiv.org/abs/2205.05256

**Understanding and documenting limitations in ML**

Alexander D'Amour, Katherine Heller, Dan Moldovan, et al. 2020. Underspecification Presents Challenges for Credibility in Modern Machine Learning. arXiv:2011.03395. Retrieved from https://arxiv.org/abs/2011.03395

Jessie J. Smith, Saleema Amershi, Solon Barocas, et al. 2022. REAL ML: Recognizing, Exploring, and Articulating Limitations of Machine Learning Research. arXiv:2205.08363. Retrieved from https://arxiv.org/abs/2205.08363

Margaret Mitchell, Simone Wu, Andrew Zaldivar, et al. 2019. Model Cards for Model Reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* '19). Association for Computing Machinery, New York, NY, USA, 220–229. https://doi.org/10.1145/3287560.3287596

Matthew Arnold, Rachel K. E. Bellamy, Michael Hind, et al. 2019. FactSheets: Increasing Trust in AI Services through Supplier's Declarations of Conformity. arXiv:1808.07261. Retrieved from https://arxiv.org/abs/1808.07261

Michael A. Madaio, Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach. 2020. Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI ‘20). Association for Computing Machinery, New York, NY, USA, 1–14. https://doi.org/10.1145/3313831.3376445

Timnit Gebru, Jamie Morgenstern, Briana Vecchione, et al. 2021. Datasheets for Datasets. arXiv:1803.09010. Retrieved from https://arxiv.org/abs/1803.09010

Bender, E. M., Friedman, B. & McMillan-Major, A.,  (2022). A Guide for Writing Data Statements for Natural Language Processing. University of Washington.  Accessed July 14, 2022. .https://techpolicylab.uw.edu/wp-content/uploads/2021/11/Data_Statements_Guide_V2.pdf

https://ai.facebook.com/blog/system-cards-a-new-resource-for-understanding-how-ai-systems-work/

**When not to deploy**

Solon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* '20). Association for Computing Machinery, New York, NY, USA, 695. https://doi.org/10.1145/3351095.3375691

**Statistical balance**

Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting racial bias in an algorithm used to manage the health of populations. Science 366, 6464 (25 Oct. 2019), 447-453. DOI: https://doi.org/10.1126/science.aax2342

**Assessment of science in AI**

Arvind Narayanan. How to recognize AI snake oil. Retrieved July 6, 2022 from https://www.cs.princeton.edu/~arvindn/talks/MIT-STS-AI-snakeoil.pdf

Emily M. Bender. 2022. On NYT Magazine on AI: Resist the Urge to be Impressed. (April 17, 2022). Retrieved July 6, 2022 from https://medium.com/@emilymenonbender/on-nyt-magazine-on-ai-resist-the-urge-to-be-impressed-3d92fd9a0edd


</details>
