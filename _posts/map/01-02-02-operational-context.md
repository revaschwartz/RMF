---
date: 01-02-02
title: MAP 2.2
categories:
  - MAP-2
description:  Information is documented about the operational context in which the AI system will be deployed (e.g., human-AI teaming, etc.) and how output will be utilized and overseen by humans.
type: Map
order_number: 2
---

{::options parse_block_html="true" /}


<details>
<summary markdown="span">**What is this subcategory about?**</summary>
<br>
Once deployed and in use, AI systems may sometimes perform poorly, manifest unanticipated negative impacts, or violate legal or ethical norms. These risks and incidents can result from a variety of factors. One key weakness stems from developing systems in highly-controlled or optimized environments that differ considerably from the deployment context. This practice may contribute to an inability to anticipate downstream uses or constraints. AI actors can reduce the likelihood of such incidents through regular stakeholder engagement and feedback. This input can provide enhanced contextual awareness about how an AI system may interact in its real-world setting. Recommended practices include broad stakeholder engagement with potentially impacted community groups, consideration of user interaction and user experience (UI/UX) factors, and regular system testing and evaluation in non-optimized conditions.

</details>

<details>
<summary markdown="span">**How can organizations achieve the outcomes of this subcategory?**</summary>

* Extend documentation beyond system and task requirements to include possible risks due to deployment contexts and human-AI configurations. 
* Follow stakeholder feedback processes to determine whether a system achieved its documented purpose within a given use context, and whether users can correctly comprehend system outputs or results.
* Document dependencies on upstream data and other AI systems, including if the specified system is an upstream dependency for another AI system or other data.
* Document connections the AI system or data will have to external networks (including the internet), financial markets, and critical infrastructure that have potential for negative externalities. Identify and document negative impacts as part of considering the broader risk thresholds and subsequent go/no-go deployment decisions.

</details>

<details>
<summary markdown="span">**What are the transparency and documentation considerations?**</summary>
<br>
**Transparency Considerations – Key Questions: MAP 2.2**
- Does the AI solution provides sufficient information to assist the personnel to make an informed decision and take actions accordingly?
- To what extent is the output of each component appropriate for the operational context?
- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals     impacted by use of the AI system?
- Based on the assessment, did your organization implement the appropriate level of human involvement in AI-augmented decision-making? (WEF Assessment)
- How will the accountable human(s) address changes in accuracy and precision due to either an adversary’s attempts to disrupt the AI or unrelated changes in operational/business environment, which may impact the accuracy of the AI?

**AI Transparency Resources: MAP 2.2**
- Datasheets for Datasets
- WEF Model AI Governance Framework Assessment 2020
- Companion to the Model AI Governance Framework- 2020
- ATARC Model Transparency Assessment (WD) – 2020
- Transparency in Artificial Intelligence - S. Larsson and F. Heintz – 2020

</details>

<details>
<summary markdown="span">**What are some informative references?**</summary>    
<br>
**Context of use**

International Standards Organization (ISO). 2019. ISO 9241-210:2019 Ergonomics of human-system interaction — Part 210: Human-centred design for interactive systems. Retrieved from https://www.iso.org/standard/77520.html

National Institute of Standards and Technology (NIST), Mary Theofanos, Yee-Yin Choong, et al. 2017. NIST Handbook 161 Usability Handbook for Public Safety Communications: Ensuring Successful Systems for First Responders. DOI: https://doi.org/10.6028/NIST.HB.161

**Human-machine interaction**

Smith, C. J. (2019). Designing trustworthy AI: A human-machine teaming framework to guide development. arXiv preprint arXiv:1910.03515.

Warden T, Carayon P, Roth EM, et al. The National Academies Board on Human System Integration (BOHSI) Panel: Explainable AI, System Transparency, and Human Machine Teaming. Proceedings of the Human Factors and Ergonomics Society Annual Meeting. 2019;63(1):631-635. doi:10.1177/1071181319631100

Committee on Human-System Integration Research Topics for the 711th Human Performance Wing of the Air Force Research Laboratory and the National Academies of Sciences, Engineering, and Medicine. 2022. Human-AI Teaming: State-of-the-Art and Research Needs. Washington, D.C. National Academies Press. DOI: https://doi.org/10.17226/26355.2022

Ben Green. 2021. The Flaws of Policies Requiring Human Oversight of Government Algorithms. Computer Law & Security Review 45 (26 Apr. 2021). DOI: https://dx.doi.org/10.2139/ssrn.3921216

Ben Green and Amba Kak. 2021. The False Comfort of Human Oversight as an Antidote to A.I. Harm. (June 15, 2021). Retrieved July 6, 2022 from https://slate.com/technology/2021/06/human-oversight-artificial-intelligence-laws.html.

Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, et al. 2021. Manipulating and Measuring Model Interpretability. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI '21). Association for Computing Machinery, New York, NY, USA, Article 237, 1–52. https://doi.org/10.1145/3411764.3445315

Susanne Gaube, Harini Suresh, Martina Raue, et al. 2021. Do as AI say: susceptibility in deployment of clinical decision-aids. npj Digital Medicine 4, Article 31 (2021). DOI: https://doi.org/10.1038/s41746-021-00385-9

Zana Buçinca, Maja Barbara Malaya, and Krzysztof Z. Gajos. 2021. To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making. Proc. ACM Hum.-Comput. Interact. 5, CSCW1, Article 188 (April 2021), 21 pages. https://doi.org/10.1145/3449287

</details>
  
